# Blue Green Deployment

- You have two environments:
 Blue = current version (live)
 Green = new version (not live yet)
- You deploy the new version to Green.
- After testing, you switch all traffic from Blue to Green at once.

‚úÖ Pros:
- Easy to rollback: just switch back to Blue.
- Staging environment exactly like production.

‚ùå Cons:
- Needs double the resources (2 full environments).
- All users get the new version at once ‚Üí higher risk if bug exists.

# u create two Deployments:
- my-app-blue with image v1
- my-app-green with image v2

üóÇ Example YAML:

# Blue deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: blue
  template:
    metadata:
      labels:
        app: my-app
        version: blue
    spec:
      containers:
      - name: my-app
        image: my-app:v1

# Green deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: green
  template:
    metadata:
      labels:
        app: my-app
        version: green
    spec:
      containers:
      - name: my-app
        image: my-app:v2
üåê Now Create a Single Service:

apiVersion: v1
kind: Service
metadata:
  name: my-app-service
spec:
  selector:
    app: my-app
    version: blue   # Start with Blue
  ports:
  - port: 80
    targetPort: 80
üîÅ Switch Traffic:

- When ready to switch to Green, just update the Service selector:

spec:
  selector:
    app: my-app
    version: green

# Most common errors to be expexted in Blue-Green Deployment:
1. Database & State Mismatch
  The Problem: If green uses a schema change or different DB version, and both blue and green are writing to the same DB, you can corrupt data.
  Why It Hurts: Rolling back to blue may fail because old code doesn‚Äôt understand new DB schema.
  Example: Green adds a new_column that blue doesn‚Äôt know about ‚Üí rollback breaks.
  Prevention: Use backward-compatible DB migrations; run migrations separately before switching.

2. Incomplete Traffic Switch
  The Problem: Even after updating the Service selector, some traffic still hits blue pods for minutes.
  Why It Hurts: Causes ‚Äúsplit brain‚Äù scenarios ‚Äî users get responses from both versions.
  Reason:
  kube-proxy caching
  DNS TTL delays
  Long-lived connections (WebSockets, gRPC) that don‚Äôt reconnect instantly.
  Prevention: Drain blue pods (graceful termination) before deleting.

3. Memory / CPU Surge on Switch
  The Problem: All traffic suddenly hitting green causes CPU/memory spike ‚Üí pods crashloop.
  Why It Hurts: Green might have passed testing but not been load-tested at scale.
  Prevention: Use load testing (k6, Locust) before cutover; set HPA (Horizontal Pod Autoscaler).

4. Config Drift Between Blue & Green
  The Problem: Environment variables, secrets, or config maps differ between deployments.
  Why It Hurts: The new app may fail for reasons unrelated to code changes.
  Prevention: Use shared ConfigMaps/Secrets, keep YAMLs templatized (Helm/Kustomize).

5. Canary Testing Is Skipped
  The Problem: Switching all traffic at once reveals major issues instantly to all users.
  Why It Hurts: You have no gradual fallback path ‚Äî rollback takes time.
  Prevention: Use Istio/NGINX Ingress to route 1‚Äì5% of traffic to green before full switch.

6. Health Checks Not Ready
  The Problem: Green pods are technically ‚Äúrunning‚Äù but fail readiness checks under load.
  Why It Hurts: Service starts sending requests to half-initialized pods ‚Üí user errors.
  Prevention: Configure readinessProbe and livenessProbe properly.

7. Rollback Is Slower Than You Think
  The Problem: ‚ÄúRollback‚Äù = Service selector change, but old pods might already be scaled down or deleted.
  Why It Hurts: You have to redeploy blue, which might take minutes.
  Prevention: Keep blue running for a fixed ‚Äúcool-off‚Äù window after cutover.